---
title: "Cluster Managment and resource allocation"
author: Ramneek Kaur <rk3994@nyu.edu>
---
# Introduction and Motivation
Effective resource allocation is one of the biggest needs among the shared computer system. In order to find a near to perfect way for cluster management in a heterogeneous environment where different users need different resources, many companies and researchers came up with different systems. One of the biggest examples is Borg, designed and used by Google for nearly a decade. Borg system works as follows as suggested in the article [Large-scale cluster management at Google with Borg](https://cs.nyu.edu/~apanda/classes/sp21/papers/borg.pdf):
  * Borg consists of numerous cells and allocates different jobs according to the priority and resources needed.
  * The workload is divided into two main categories called “prod”, stands for production and “non-prod”, stands for non-production.  Prod jobs have highest priority since it’s supposed to be continuous. Since the workload is divided in these sets scheduling becomes much easier.

Furthermore, the traces of 2019 data from Borg showed some differences from the original Borg traces of 2011. The new changes observed were referred to as [Borg the next generation](https://cs.nyu.edu/~apanda/classes/sp21/papers/borgv2.pdf). The major changes were seen in the job’s priority and in the resource consumption of big and small jobs.
 
Other than Borg, max-min fairness is also a popular allocation policy. Max-min fairness maximizes the minimum share to each user in the system. The idea behind the Max-min fairness system is that resources should be shared equally in a fair way among all users. A different strategy called [Dominant Resource fairness (DRF)](https://cs.nyu.edu/~apanda/classes/sp21/papers/drf.pdf) extended the max-min fairness policy in the following ways:
 
* DRF ensures that no user is better off if resources are equally distributed.
* Users cannot increase allocation by lying about its usage.
* No user would trade allocation with another user.

The motivation behind DRF is to extend max-min fairness to meet heterogeneous demands of users and multiple resources since max-min fairness policy only deals with single resources. Both Borg and DRF give two different solutions to deal with one problem of effective cluster management and resource allocation in heterogeneous environments according to the users demands.

# Approaches
## Borg
The main architecture of Borg consists of numerous cells which contains a set of machines, Borgmaster and Borglet as seen in the diagram below. The entire workload takes place inside these cells. 
![alt text](https://github.com/Ramneek99/notes-2021/blob/main/feb23/Screen%20Shot%202021-02-28%20at%208.16.22%20PM.png)

* Every job to be entered in the cell has a priority value. The priority level helps to accommodate the important jobs first and if needed on the cost of already running low priority jobs. Ranges of priorities are called tiers and are divided as follows:
  * Free Tier: Lowest priority and low internal charges.
  * Best-effort Batch (beb) tier: more priority than Free tier. Jobs run by batch scheduler and low internal charges.
  * Mid-tier: Low internal charge and higher priority than Beb tier. Mid tier was introduced in the traces of 2019.
  * Production tier: Jobs require high availability and full internal charges. High priority.
  * Monitoring tier: This tire includes jobs that are running to monitor other jobs. For this paper, this tier is counted towards the production tier.
* Task allocation depends on the priority level.
* Inside the cell Borgmaster is responsible for managing and keeping record of the resources and monitoring the cell as well. 
* Borg master replicates itself into multiple copies to keep record of the work. 
* While sending the work into the Paxos store, Borgmaster picks some of its replicas and replicates the job given into them. This helps maintain a previous job in case BorgMaster experiences failure.
* After the work is sent to Paxos store, the scheduler decides which task will be given to which Borglet according to its need of resources. Scheduler also decides which task will be sent first according to its priority level. 
* Once the machine receives the task, Borglet is responsible for starting and stopping the tasks, and restart them if any tasks fail. Borglet also keeps a log of each task and reports the state of the machine to Borg master and other monitoring machines. If a Borglet does not respond then Borgmaster marks it as down and if communication would ever be restored, Borgmaster kills all the tasks.

## Borg the next generation
In the new traces of data in 2019, some major difference were seen from the original traces of 2011, which are as follows:
* The traces for 2019 covers eight different cells but traces for 2011 covered only one cell.
* New traces depict several new features including information about allocation and parent-child dependencies. 
* The workload has increased but the cell size remains constant.
* The scheduling rate is higher since the job admission rate has been increased by 3.7 time higher than 2011 traces. 
* Job priority values has been changed to sparse values in the range 0-450 whereas in 2011, the job priority values were integers ranging from 0-11.
Changes in utilization of resources were seen as shown in the graphs below:
![alt text](https://github.com/Ramneek99/notes-2021/blob/main/feb23/Screen%20Shot%202021-03-01%20at%201.33.15%20PM.png)
  * As suggested by figure 2 above, the CPU and memory usage has been increased (note that 2011 graphs are for one cell and 2019 graphs are for 8 cells).
  * Beb tier jobs take comparatively high CPU and memory usage in 2019 traces as compared to 2011 whereas prod tier usage has been approximately unchanged.
  ![alt text](https://github.com/Ramneek99/notes-2021/blob/main/feb23/Screen%20Shot%202021-03-01%20at%201.34.39%20PM.png)
  * In figure 4, CPU and memory allocation has been slightly changed for prod tier but drastically increased for beb tier whereas free tier have also increased to      some point. The primary reason is related to many jobs running under the beb tier.
  
## Dominant Resource Fairness(DRF)
* DRF is a new allocation policy extending the Max-min fairness. Max-min fairness policy is based on the fair allocation of resources by making sure that each work gets its share of resources needed without wasting any.
* The four properties used to ensure fair allocation of resources are as follows:
  * Share incentive: Each user should share a cluster rather than using their own partition of the cluster.
  * Strategy-proofness: No user should get any benefit from lying about their demands.
  * Envy-fairness: No user is allowed to exchange allocation of another user.
  * Pareto efficiency: No user should be able to increase its allocation without decreasing another user’s allocation.
* Max-min fairness generally focuses on single resources which is why DRF introduces a new way to promote fair allocation in multiple resources. 
* DRF computes the share of each resource allocated to the user in order to find the dominant share, i.e., the maximum shares among all users.
* After that, DRF applies Max-min fairness across users' dominant share and equally distributes the remaining resources to the users. 
* For example, for two users if one user’s dominant resource is memory and other user’s dominant resource is CPU then DRF will divide the memory by giving user A more memory resources as compared to user B and user B will get more CPU resources comparatively user A.


# Trade-Offs
## Borg and next generation Borg
### Limitations
* Borg has no direct way to handle multi job service as a single entity.
* One machine has only one IP address for all the tasks which make things complicated by sharing the host’s port space.
* Borg optimizes the power user on the expense of casual ones which becomes problematic and hard for casual users.

## Dominant Resource Allocation (DRF)
### Limitations
As suggested by [project report](http://www.cs.tau.ac.il/~fiat/cgt12/report-yael.pdf) by Yael Amsterdamer, the DRF strategy proof algorithm does not give good results for discrete case(when any fraction of task can be executed):
* DRF is not envy free in discrete cases.
* In discrete cases sharing incentive does not hold true.
* DRF is not incentive compatible in discrete cases.

## Comparison between approaches:
* DRF only allocates minimum resources to run the jobs and reduces not needed resources. In contrast to Borg where users can specify a minimum amount of resources for the job even if the job is currently not using all of it. 
* DRF doesn’t support priority of the jobs or have a priority tier compared to the Borg. Priority is helpful because it can make sure that free or test jobs don't get too many resources allocation. It can starve production and high priority jobs. 
* Due to Borgs prioritization of jobs, it becomes hard for less priority jobs to get resources which is a disadvantage when traffic increases.
* Jobs maintenance falls onto developer and has no built in ways to maintain jobs across multiple allocs


# Open Questions and Future Work
## Borg
* The traces from 2011 and 2019 Borg cell are unable to clarify how scheduling makes decisions it made which it made.
* Borg algorithm is greedy heuristics in which a job will be started as soon as any of its tasks are running. Is there any way it could be better?
* As seen in traces of 2019 comparatively traces of 2011, the average utilization of computer and memory resources is low, which is not clarified.
* How can we improve scheduling so that "hogs"(intense jobs) and "mice"(not intense jobs) have a barrier which would decrease the loading time for mice.

## DRF
* Minimizing resource fragmentation without compromising fairness during discrete tasks are important and yet to be achieved.
* Defining fairness is challenging when tasks have placement constraints.
* DRF algorithms still have not been in use as an operating system.

